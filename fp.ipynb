{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš¨ Any changes here will be overwritten by git. Keep in mind when making any editions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82V6cZ2M-f0-"
   },
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRuwUZRbLBLc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from google.colab import drive\n",
    "from google.colab import userdata\n",
    "\n",
    "#TODO: add rsync from here to GitHub pre-commit hook (it's tempting to edit the file here)\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "try:\n",
    "  MODEL_TRAINING_PATH = userdata.get('MODEL_TRAINING_PATH')\n",
    "except userdata.SecretNotFoundError as e:\n",
    "    print(\n",
    "        \"Error: Path to shared model training not found, please point to it. \\n\"\n",
    "        \"Should be something like: /content/drive/My Drive/ut/nlp_final/model_training\"\n",
    "        \"The path should be a shortcut to this folder https://drive.google.com/drive/folders/1kyZuHKEu0cc-VFNJvxo0CK0poBFeesXz ,\\n\"\n",
    "        \" stored in your local GDrive. \\n\"\n",
    "        \"Exiting...\"\n",
    "    )\n",
    "    #TODO: not really nice output not sure how to make better\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "os.environ['MODEL_TRAINING_PATH'] = MODEL_TRAINING_PATH\n",
    "\n",
    "# Import secrets\n",
    "os.environ['WANDB_API_KEY']=userdata.get('WANDB_API_KEY')\n",
    "\n",
    "if not userdata.get('WANDB_API_KEY'):\n",
    "    print(\"Error: WANDB_API_KEY is missing or empty. It can be retrieved from https://wandb.ai/authorize. Exiting...\")\n",
    "    exit  # Exit the notebook with an error code\n",
    "\n",
    "# Auth user\n",
    "try:\n",
    "  USER = userdata.get('USER')\n",
    "except userdata.SecretNotFoundError as e:\n",
    "    print(\n",
    "        \"Error. Add your name to the secrets (quicker than google auth each time).\"\n",
    "    )\n",
    "    #TODO: not really nice output not sure how to make better\n",
    "    sys.exit(0)\n",
    "\n",
    "print(\"User: \", USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "vzahVV1Q-8-7"
   },
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "# Check if the repository already exists\n",
    "%cd /content\n",
    "BRANCH=\"main\"\n",
    "!if [ ! -d \"fp-dataset-artifacts\" ]; then \\\n",
    "    echo \"Repository not found. Cloning...\"; \\\n",
    "    git clone -b $BRANCH https://github.com/pkey/fp-dataset-artifacts.git; \\\n",
    "else \\\n",
    "    echo \"Repository already exists. Pulling latest changes...\"; \\\n",
    "    cd fp-dataset-artifacts && git checkout $BRANCH && git pull origin $BRANCH; \\\n",
    "fi\n",
    "\n",
    "%cd fp-dataset-artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zbbd8qde8Xr6"
   },
   "outputs": [],
   "source": [
    "# Initialise colab environment\n",
    "!make initialise/colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSkF1PJQFBpV"
   },
   "source": [
    "# Training or Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAMDXvNo8wdR"
   },
   "outputs": [],
   "source": [
    "# Train. You can use whatever command, either from Makefile or directly. MAKE SURE TO RUN THE PREP STEPS (or run all), Command + F9.\n",
    "\n",
    "# Choose if you want to do both or only one\n",
    "TRAINING = False\n",
    "EVALUATION = False\n",
    "EVALUATION_BASE_SQUAD = False\n",
    "\n",
    "if not TRAINING and not EVALUATION:\n",
    "    print(\"Please choose one of training or evaluation to proceed\")\n",
    "    sys.exit(0)\n",
    "\n",
    "current_date_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "EXPERIMENT_NAME = f\"{USER}-{current_date_time}\"\n",
    "os.environ['WANDB_NAME'] = EXPERIMENT_NAME\n",
    "print(\"Experiment name: \", EXPERIMENT_NAME)\n",
    "\n",
    "os.environ['WANDB_PROJECT'] = \"NLP Final Project 2024\"\n",
    "\n",
    "# NOTE: Add here a small note on what changed or what is special about this experiment\n",
    "os.environ['WANDB_NOTES']= input(\"Your experiment notes: \")\n",
    "\n",
    "# NOTE: Depending on GPU, can experiment\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE=60\n",
    "\n",
    "# We are working with squad / squad_v2\n",
    "DATASET = \"squad\"\n",
    "\n",
    "# Choose a different subtype for trained electra model (small, base, large)\n",
    "ELECTRA_SUBTYPE = \"small\"\n",
    "\n",
    "# We are using QA mostly so this one should stay unchanged\n",
    "TASK = \"qa\"\n",
    "\n",
    "MODEL_PATH = f\"{MODEL_TRAINING_PATH}/{EXPERIMENT_NAME}_electra-{ELECTRA_SUBTYPE}-{DATASET}\"\n",
    "\n",
    "\n",
    "if (TRAINING):\n",
    "    print(\"Model will be saved at: \", MODEL_PATH)\n",
    "    !python3 run.py --do_train --task $TASK --dataset $DATASET --output_dir \"{MODEL_PATH}\" --per_device_train_batch_size $PER_DEVICE_TRAIN_BATCH_SIZE\n",
    "else:\n",
    "    print(\"Skipping training...\")\n",
    "\n",
    "#TODO: we might not always want to eval the same model. Arrange params in a nicer way here.\n",
    "if EVALUATION:\n",
    "    if EVALUATION_BASE_SQUAD:\n",
    "        MODEL_PATH = f\"{MODEL_TRAINING_PATH}/trained_model_electra_{ELECTRA_SUBTYPE}_{DATASET}\"\n",
    "    \n",
    "    !python3 run.py --do_eval --task $TASK --dataset $DATASET --model \"{MODEL_PATH}\" --output_dir \"{MODEL_TRAINING_PATH}/eval_{EXPERIMENT_NAME}\"\n",
    "else:\n",
    "    print(\"Skipping evaluation...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis (can be run locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered predictions saved at: /Users/pauliuskutka/Library/CloudStorage/GoogleDrive-kutka.paulius@gmail.com/My Drive/ut/nlp_final/model_training/eval_test/filtered_predictions_correct.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# Locally, run make initialise/local to make sure .env is available\n",
    "MODEL_TRAINING_PATH = os.getenv(\"MODEL_TRAINING_PATH\")\n",
    "\n",
    "# Analysis of output predictions (can bu run locally as well)\n",
    "path_to_predictions=f\"{MODEL_TRAINING_PATH}/custom_datasets/eval_predictions.jsonl\"\n",
    "path_to_filtered = f\"{MODEL_TRAINING_PATH}/custom_datasets/all_squad_predictions.csv\"\n",
    "\n",
    "df = pd.read_json(path_to_predictions, lines=True)\n",
    "\n",
    "# Filter rows where there isn't an exact match in the answers_text list\n",
    "df = df[df.apply(lambda row: row[\"overlap_score\"] == 1, axis=1)]\n",
    "\n",
    "\n",
    "# You can view the csv in Google Sheets in google drive (will create a new sheet)\n",
    "df.to_json(path_to_filtered, lines=True, orient=\"records\")\n",
    "print(f\"Filtered predictions saved at: {path_to_filtered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonl to CSV\n",
    "import pandas as pd\n",
    "import os\n",
    "# Locally, run make initialise/local to make sure .env is available\n",
    "MODEL_TRAINING_PATH = os.getenv(\"MODEL_TRAINING_PATH\")\n",
    "\n",
    "filename = \"eval_predictions\"\n",
    "\n",
    "path_to_jsonl = f\"model_training/eval_output_squad_when_experiment/{filename}.jsonl\"\n",
    "path_to_csv = f\"model_training/eval_output_squad_when_experiment/{filename}.csv\" \n",
    "\n",
    "df = pd.read_json(path_to_jsonl, lines=True)\n",
    "df.to_csv(path_to_csv, index=False, header=True, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet to CSV\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Locally, run make initialise/local to make sure .env is available\n",
    "MODEL_TRAINING_PATH = os.getenv(\"MODEL_TRAINING_PATH\")\n",
    "\n",
    "filename = \"validation_squad\"\n",
    "\n",
    "path_to_parquet = f\"{MODEL_TRAINING_PATH}/custom_datasets/{filename}.parquet\"\n",
    "path_to_csv = f\"{MODEL_TRAINING_PATH}/custom_datasets/{filename}.csv\" \n",
    "\n",
    "df = pd.read_parquet(path_to_parquet)\n",
    "df.to_csv(path_to_csv, index=False, header=True, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV columns to jsonl\n",
    "# NOTE: the 'answer' column gets messed up when converted to CSV.\n",
    "# Therefore, we only handpick columns that we want to sync back to jsonl (or new jsonl)\n",
    "\n",
    "# jsonl to CSV\n",
    "import pandas as pd\n",
    "import os\n",
    "# Locally, run make initialise/local to make sure .env is available\n",
    "MODEL_TRAINING_PATH = os.getenv(\"MODEL_TRAINING_PATH\")\n",
    "\n",
    "jsonl_filename_existing = \"eval_predictions_correct\" \n",
    "jsonl_filename_new = \"eval_predictions_correct_sarcastic\"\n",
    "csv_filename = \"eval_predictions_correct\"\n",
    "\n",
    "path_to_jsonl_existing = f\"{MODEL_TRAINING_PATH}/custom_datasets/{jsonl_filename_existing}.jsonl\"\n",
    "path_to_jsonl_new = f\"{MODEL_TRAINING_PATH}/custom_datasets/{jsonl_filename_new}.jsonl\"\n",
    "path_to_csv = f\"{MODEL_TRAINING_PATH}/custom_datasets/{csv_filename}.csv\" \n",
    "\n",
    "df_csv = pd.read_csv(path_to_csv)\n",
    "df_jsonl = pd.read_json(path_to_jsonl_existing, lines=True)\n",
    "\n",
    "df_jsonl[\"context\"] = df_csv[\"context\"]\n",
    "\n",
    "df_jsonl.to_json(path_to_jsonl_new, lines=True, orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorise using GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorise using GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate adversarial data using chat GPT to test event-based time QA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response generated!\n",
      "Response generated!\n",
      "Failed for question: When were the two finalists for hosting Super Bowl 50 announced?. Ignoring. Error: Expecting value: line 1 column 1 (char 0)\n",
      "Response generated!\n",
      "Response generated!\n",
      "Failed for question: When did the San Francisco Bay area last host the Super Bowl?. Ignoring. Error: Expecting value: line 1 column 1 (char 0)\n",
      "Response generated!\n",
      "Failed for question: When were the finalists announced?. Ignoring. Error: Expecting value: line 1 column 1 (char 0)\n",
      "Response generated!\n",
      "Failed for question: When was the last time San Francisco hosted a Super Bowl?. Ignoring. Error: Expecting value: line 1 column 1 (char 0)\n",
      "Response generated!\n",
      "Response generated!\n",
      "Response generated!\n",
      "Response generated!\n",
      "Response generated!\n",
      "Response generated!\n",
      "Response generated!\n",
      "Response generated!\n",
      "Response generated!\n",
      "Response generated!\n",
      "Failed for question: When did the Packers arrive at a record of 13-0?. Ignoring. Error: Expecting value: line 1 column 1 (char 0)\n",
      "Response generated!\n",
      "Response generated!\n",
      "Failed for question: When did the league go from 15 to 16 games in the regular season?. Ignoring. Error: Expecting value: line 1 column 1 (char 0)\n",
      "Response generated!\n",
      "Failed for question: When John Fox left as head coach for the Broncos, who replaced him?. Ignoring. Error: Expecting value: line 1 column 1 (char 0)\n",
      "Response generated!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI, ChatCompletion\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "MODEL_TRAINING_PATH = os.getenv(\"MODEL_TRAINING_PATH\")\n",
    "path_to_predictions = f\"{MODEL_TRAINING_PATH}/custom_datasets/eval_predictions.jsonl\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
    ")\n",
    "\n",
    "# Read the evaluation predictions\n",
    "df = pd.read_json(path_to_predictions, lines=True)\n",
    "\n",
    "# Filter rows that were predicted right\n",
    "df = df[df[\"overlap_score\"] == 1]\n",
    "\n",
    "# Function to categorize errors using GPT\n",
    "def generate_data_based_on_row(row):\n",
    "    prompt = f\"\"\"\n",
    "    Given the following example:\n",
    "    \n",
    "    Title: {row[\"title\"]}\n",
    "    Context: {row[\"context\"]}\n",
    "    Correct Answers: {row[\"answers\"][\"text\"]}\n",
    "    Question: {row[\"question\"]}\n",
    "\n",
    "    and if the correct answer contains any form of date (or other specific time event), replace it with a related (can be imaginary)\n",
    "    event. Try not not modify the context as much as possible. Make sure tha the answer can be directly found\n",
    "    int the context as it appears. Keep questions exactly the same.\n",
    "    \n",
    "    Please return the data as a JSON object, not enclosed in code blocks or any additional text, with this exact format:\n",
    "    {{\n",
    "        \"title\": <same-title>\n",
    "        \"context\": <modified-context>,\n",
    "        \"answer\": <new-correct-answer>,\n",
    "        \"question\": <unchanged-original-question>\n",
    "    }}\n",
    "    \n",
    "    Here are a few examples:\n",
    "\n",
    "    a) first example\n",
    "    \n",
    "    given:\n",
    "    \n",
    "    Title: Super_Bowl_50\n",
    "    Context: In early 2012, NFL Commissioner Roger Goodell stated that the league planned to make the 50th Super Bowl \"spectacular\" and that it would be \"an important game for us as a league\".\n",
    "    Correct Answers: {{'text': ['early 2012', 'In early 2012', '2012'], 'answer_start': [3, 0, 9]}}\n",
    "    Question: When did he make the quoted remarks about Super Bowl 50?\n",
    "    \n",
    "    you would return:\n",
    "    \n",
    "    {{\n",
    "        \"title\": \"Super_Bowl_50\"\n",
    "        \"context\": \"During the winter season, NFL Commissioner Roger Goodell stated that the league planned to make the 50th Super Bowl \"spectacular\" and that it would be \"an important game for us as a league\"\n",
    "        \"answer\": \"During the winter season\"\n",
    "        \"question\": \"When did he make the quoted remarks about Super Bowl 50?\" \n",
    "    }}\n",
    "    \n",
    "    b) second example\n",
    "    \n",
    "    given:\n",
    "    \n",
    "    Title: Normans\n",
    "    Context: The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrowed from Old Low Franconian Nortmann \"Northman\" or directly from Old Norse NorÃ°maÃ°r, Latinized variously as Nortmannus, Normannus, or Nordmannus (recorded in Medieval Latin, 9th century) to mean \"Norseman, Viking\".\n",
    "    Correct Answers: {{'text': ['9th century', '9th century', '9th century'], 'answer_start': [309, 309, 309]}}\n",
    "    Question: When was the Latin version of the word Norman first recorded?\n",
    "    \n",
    "    you would return:\n",
    "    \n",
    "    {{\n",
    "        \"title\": \"Normans\"\n",
    "        \"context\": \"The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrowed from Old Low Franconian Nortmann \"Northman\" or directly from Old Norse NorÃ°maÃ°r, Latinized variously as Nortmannus, Normannus, or Nordmannus (recorded in Medieval Latin, around the same time Pope Francis Second was in reign) to mean \"Norseman, Viking\"\"\n",
    "        \"answer\": \"around the same time Pope Francis Second was in reign\"\n",
    "        \"question\": \"When was the Latin version of the word Norman first recorded?\" \n",
    "    }} \n",
    "    \n",
    "    \"\"\"\n",
    "    response: ChatCompletion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specializing in NLP data generation.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    )\n",
    "    print(\"Response generated!\")\n",
    "    new_row = {}\n",
    "    stripped_response = response.choices[0].message.content.strip()\n",
    "\n",
    "    try:\n",
    "        structured_response = json.loads(stripped_response)\n",
    "        \n",
    "        if (structured_response[\"question\"] != row[\"question\"].strip()):\n",
    "            raise Exception(\"Questions not equal, skip the rest.\")\n",
    "        \n",
    "        new_row[\"title\"] = structured_response[\"title\"]\n",
    "        new_row[\"context\"] = structured_response[\"context\"]\n",
    "        new_row[\"question\"] = structured_response[\"question\"]\n",
    "        new_row[\"answer\"] = structured_response[\"answer\"] \n",
    "    except Exception as e:  # noqa: E722\n",
    "        print(f\"Failed for question: {row[\"question\"]}. Ignoring. Error: {e}\")\n",
    "   \n",
    "    return new_row\n",
    "    \n",
    "# Filter out When questions\n",
    "df = df[df['question'].str.startswith(\"When\")]\n",
    "\n",
    "# Apply categorization and extract structured data\n",
    "df = df[:20]\n",
    "new_columns = list(df.columns) + [\"answer\"]\n",
    "df_new = pd.DataFrame(columns=new_columns)\n",
    "for index, row in df.iterrows():\n",
    "    modified_row = generate_data_based_on_row(row)\n",
    "    if modified_row:\n",
    "        df_new.loc[len(df_new.index)] = modified_row\n",
    "\n",
    "# Save the updated DataFrame with categories\n",
    "df_new.to_csv(f\"{MODEL_TRAINING_PATH}/gpt_generated_data.csv\", index=False, header=True, sep=',', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI, ChatCompletion\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "MODEL_TRAINING_PATH = os.getenv(\"MODEL_TRAINING_PATH\")\n",
    "path_to_predictions = f\"{MODEL_TRAINING_PATH}/eval_predictions.jsonl\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
    ")\n",
    "\n",
    "# Read the evaluation predictions\n",
    "df = pd.read_json(path_to_predictions, lines=True)\n",
    "\n",
    "# Filter rows with overlap_score = 0\n",
    "df_zero_score = df[df[\"overlap_score\"] == 0]\n",
    "\n",
    "# Function to categorize errors using GPT\n",
    "def categorize_error(row):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following information and categorize why the predicted answer might be wrong:\n",
    "\n",
    "    Context: {row[\"context\"]}\n",
    "    Predicted Answer: {row[\"predicted_answer\"]}\n",
    "    Correct Answers: {row[\"answers\"][\"text\"]}\n",
    "\n",
    "    Some of the categories that can be:\n",
    "        - Pronoun Resolution Error\n",
    "        - Temporal Reasoning Error\n",
    "\n",
    "    Don't limit yourself to these categories and come with with industry standard ones. \n",
    "    \n",
    "    Provide the most relevant category and a brief explanation. Please return the data as a JSON object, not enclosed in code blocks or any additional text, with this exact format:\n",
    "    {{\n",
    "        \"category\": \"Category Name\",\n",
    "        \"explanation\": \"A brief explanation of why the model made this error.\"\n",
    "    }}\n",
    "    \n",
    "    \"\"\"\n",
    "    response: ChatCompletion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specializing in NLP evaluation.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    )\n",
    "    #TODO: this part is flimsy, doesn't properly split\n",
    "    stripped_response = response.choices[0].message.content.strip()\n",
    "    structured_response = json.loads(stripped_response)\n",
    "    return structured_response\n",
    "\n",
    "# Apply categorization and extract structured data\n",
    "categorized = df_zero_score.apply(categorize_error, axis=1)\n",
    "df_zero_score[\"error_category\"] = categorized.apply(lambda x: x.get(\"category\", \"Error\"))\n",
    "df_zero_score[\"error_explanation\"] = categorized.apply(lambda x: x.get(\"explanation\", \"No explanation provided\"))\n",
    "\n",
    "# Save the updated DataFrame with categories\n",
    "df_zero_score.to_csv(f\"{MODEL_TRAINING_PATH}/categorized_errors.csv\", index=False, header=True, sep=',', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "cell_execution_strategy": "setup",
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ut_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
