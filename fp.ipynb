{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš¨ Any changes here will be overwritten by git. Keep in mind when making any editions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82V6cZ2M-f0-"
   },
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRuwUZRbLBLc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from google.colab import drive\n",
    "from google.colab import userdata\n",
    "\n",
    "#TODO: add rsync from here to GitHub pre-commit hook (it's tempting to edit the file here)\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "try:\n",
    "  MODEL_TRAINING_PATH = userdata.get('MODEL_TRAINING_PATH')\n",
    "except userdata.SecretNotFoundError as e:\n",
    "    print(\n",
    "        \"Error: Path to shared model training not found, please point to it. \\n\"\n",
    "        \"Should be something like: /content/drive/My Drive/ut/nlp_final/model_training\"\n",
    "        \"The path should be a shortcut to this folder https://drive.google.com/drive/folders/1kyZuHKEu0cc-VFNJvxo0CK0poBFeesXz ,\\n\"\n",
    "        \" stored in your local GDrive. \\n\"\n",
    "        \"Exiting...\"\n",
    "    )\n",
    "    #TODO: not really nice output not sure how to make better\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "os.environ['MODEL_TRAINING_PATH'] = MODEL_TRAINING_PATH\n",
    "\n",
    "# Import secrets\n",
    "os.environ['WANDB_API_KEY']=userdata.get('WANDB_API_KEY')\n",
    "\n",
    "if not userdata.get('WANDB_API_KEY'):\n",
    "    print(\"Error: WANDB_API_KEY is missing or empty. It can be retrieved from https://wandb.ai/authorize. Exiting...\")\n",
    "    exit  # Exit the notebook with an error code\n",
    "\n",
    "# Auth user\n",
    "try:\n",
    "  USER = userdata.get('USER')\n",
    "except userdata.SecretNotFoundError as e:\n",
    "    print(\n",
    "        \"Error. Add your name to the secrets (quicker than google auth each time).\"\n",
    "    )\n",
    "    #TODO: not really nice output not sure how to make better\n",
    "    sys.exit(0)\n",
    "\n",
    "print(\"User: \", USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "vzahVV1Q-8-7"
   },
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "# Check if the repository already exists\n",
    "%cd /content\n",
    "BRANCH=\"main\"\n",
    "!if [ ! -d \"fp-dataset-artifacts\" ]; then \\\n",
    "    echo \"Repository not found. Cloning...\"; \\\n",
    "    git clone -b $BRANCH https://github.com/pkey/fp-dataset-artifacts.git; \\\n",
    "else \\\n",
    "    echo \"Repository already exists. Pulling latest changes...\"; \\\n",
    "    cd fp-dataset-artifacts && git checkout $BRANCH && git pull origin $BRANCH; \\\n",
    "fi\n",
    "\n",
    "%cd fp-dataset-artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zbbd8qde8Xr6"
   },
   "outputs": [],
   "source": [
    "# Initialise colab environment\n",
    "!make initialise/colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSkF1PJQFBpV"
   },
   "source": [
    "# Training or Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAMDXvNo8wdR"
   },
   "outputs": [],
   "source": [
    "# Train. You can use whatever command, either from Makefile or directly. MAKE SURE TO RUN THE PREP STEPS (or run all), Command + F9.\n",
    "\n",
    "# Choose if you want to do both or only one\n",
    "TRAINING = False\n",
    "EVALUATION = False\n",
    "EVALUATION_BASE_SQUAD = False\n",
    "\n",
    "if not TRAINING and not EVALUATION:\n",
    "    print(\"Please choose one of training or evaluation to proceed\")\n",
    "    sys.exit(0)\n",
    "\n",
    "current_date_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "EXPERIMENT_NAME = f\"{USER}-{current_date_time}\"\n",
    "os.environ['WANDB_NAME'] = EXPERIMENT_NAME\n",
    "print(\"Experiment name: \", EXPERIMENT_NAME)\n",
    "\n",
    "os.environ['WANDB_PROJECT'] = \"NLP Final Project 2024\"\n",
    "\n",
    "# NOTE: Add here a small note on what changed or what is special about this experiment\n",
    "os.environ['WANDB_NOTES']= input(\"Your experiment notes: \")\n",
    "\n",
    "# NOTE: Depending on GPU, can experiment\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE=60\n",
    "\n",
    "# We are working with squad / squad_v2\n",
    "DATASET = \"squad\"\n",
    "\n",
    "# Choose a different subtype for trained electra model (small, base, large)\n",
    "ELECTRA_SUBTYPE = \"small\"\n",
    "\n",
    "# We are using QA mostly so this one should stay unchanged\n",
    "TASK = \"qa\"\n",
    "\n",
    "MODEL_PATH = f\"{MODEL_TRAINING_PATH}/{EXPERIMENT_NAME}_electra-{ELECTRA_SUBTYPE}-{DATASET}\"\n",
    "\n",
    "\n",
    "if (TRAINING):\n",
    "    print(\"Model will be saved at: \", MODEL_PATH)\n",
    "    !python3 run.py --do_train --task $TASK --dataset $DATASET --output_dir \"{MODEL_PATH}\" --per_device_train_batch_size $PER_DEVICE_TRAIN_BATCH_SIZE\n",
    "else:\n",
    "    print(\"Skipping training...\")\n",
    "\n",
    "#TODO: we might not always want to eval the same model. Arrange params in a nicer way here.\n",
    "if EVALUATION:\n",
    "    if EVALUATION_BASE_SQUAD:\n",
    "        MODEL_PATH = f\"{MODEL_TRAINING_PATH}/trained_model_electra_{ELECTRA_SUBTYPE}_{DATASET}\"\n",
    "    \n",
    "    !python3 run.py --do_eval --task $TASK --dataset $DATASET --model \"{MODEL_PATH}\" --output_dir \"{MODEL_TRAINING_PATH}/eval_{EXPERIMENT_NAME}\"\n",
    "else:\n",
    "    print(\"Skipping evaluation...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis (can be run locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump incorrect predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect answers saved to: /Users/pauliuskutka/Library/CloudStorage/GoogleDrive-kutka.paulius@gmail.com/My Drive/ut/nlp_final/model_training/eval_test/filtered_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# Locally, run make initialise/local to make sure .env is available\n",
    "MODEL_TRAINING_PATH = os.getenv(\"MODEL_TRAINING_PATH\")\n",
    "\n",
    "# Analysis of output predictions (can bu run locally as well)\n",
    "# NOTE: Change the path to your evaluation:\n",
    "EVALUATION_PATH=f\"{MODEL_TRAINING_PATH}/eval_squad\"\n",
    "\n",
    "df = pd.read_json(f\"{EVALUATION_PATH}/eval_predictions.jsonl\", lines=True)\n",
    "\n",
    "# Filter rows where there isn't an exact match in the answers_text list\n",
    "df_filtered = df[df.apply(lambda row: row[\"overlap_score\"] < 1, axis=1)]\n",
    "\n",
    "\n",
    "# You can view the csv in Google Sheets in google drive (will create a new sheet)\n",
    "df_filtered.to_csv(f\"{EVALUATION_PATH}/filtered_predictions.csv\", index=False, header=True, sep=',', encoding='utf-8')\n",
    "print(f\"Incorrect answers saved to: {EVALUATION_PATH}/filtered_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorise using GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI, ChatCompletion\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "MODEL_TRAINING_PATH = os.getenv(\"MODEL_TRAINING_PATH\")\n",
    "EVALUATION_PATH = f\"{MODEL_TRAINING_PATH}/eval_test\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
    ")\n",
    "\n",
    "# Read the evaluation predictions\n",
    "df = pd.read_json(f\"{EVALUATION_PATH}/eval_predictions.jsonl\", lines=True)\n",
    "\n",
    "# Filter rows with overlap_score = 0\n",
    "df_zero_score = df[df[\"overlap_score\"] == 0]\n",
    "\n",
    "# Function to categorize errors using GPT\n",
    "def categorize_error(row):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following information and categorize why the predicted answer might be wrong:\n",
    "\n",
    "    Context: {row[\"context\"]}\n",
    "    Predicted Answer: {row[\"predicted_answer\"]}\n",
    "    Correct Answers: {row[\"answers\"][\"text\"]}\n",
    "\n",
    "    Some of the categories that can be:\n",
    "        - Pronoun Resolution Error\n",
    "        - Temporal Reasoning Error\n",
    "\n",
    "    Don't limit yourself to these categories and come with with industry standard ones. \n",
    "    \n",
    "    Provide the most relevant category and a brief explanation. Please return the data as a JSON object, not enclosed in code blocks or any additional text, with this exact format:\n",
    "    {{\n",
    "        \"category\": \"Category Name\",\n",
    "        \"explanation\": \"A brief explanation of why the model made this error.\"\n",
    "    }}\n",
    "    \n",
    "    \"\"\"\n",
    "    response: ChatCompletion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specializing in NLP evaluation.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    )\n",
    "    #TODO: this part is flimsy, doesn't properly split\n",
    "    stripped_response = response.choices[0].message.content.strip()\n",
    "    structured_response = json.loads(stripped_response)\n",
    "    return structured_response\n",
    "\n",
    "# Apply categorization and extract structured data\n",
    "categorized = df_zero_score.apply(categorize_error, axis=1)\n",
    "df_zero_score[\"error_category\"] = categorized.apply(lambda x: x.get(\"category\", \"Error\"))\n",
    "df_zero_score[\"error_explanation\"] = categorized.apply(lambda x: x.get(\"explanation\", \"No explanation provided\"))\n",
    "\n",
    "# Save the updated DataFrame with categories\n",
    "df_zero_score.to_csv(f\"{EVALUATION_PATH}/categorized_errors.csv\", index=False, header=True, sep=',', encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "cell_execution_strategy": "setup",
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ut_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
